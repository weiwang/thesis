---
title: Insensitivity of Predictive Accuracy for Selecting among Multilevel Models
author:
- name: Wei Wang
  affiliation: Columbia University
  email: ww2243@columbia.edu
date: Octorber, 2015
abstract:
  As a simple and compelling approach for estimating out-of-sample prediction
  error, cross-validation naturally lends itself to the task of model
  comparison. However, even with moderate sample size, it can be surprisingly
  difficult to compare multilevel models based on predictive accuracy.  Using a
  hierarchical model fit to large survey data with a battery of questions, we
  demonstrate that even though cross-validation might give good estimates of
  pointwise out-of-sample prediction error, it is not always a sensitive
  instrument for model comparison.
bibliography: crossval.bib
...

Models selection is an integral part of any data analysis. In an ideal world,
iteratively improving and comparing model fits of different specifications
should be the routine of all statistical procedures, especailly when
developments in methodology and computation facilitate evermore sophisticated
and complex models. Often, the most important question is not that whether a
more complicated model is computational tractable, but why this model is an
improvement over the older and simpler ones. Multilevel models (also known as
Hierarchical Models) are an example of modern statistical models, which
specifically handles data with group structure, for example, a national survey
data with geographic and demographic information or an educational intervention
applied to different schools and neighborhoods.

The gold standard of model comparison is out-of-sample prediction accuracy,
i.e., in the hypothetical case of more observations coming in, which model gives
the best prediction of new case of outcomes based on new cases of
predictors. Cross-validation is a perhaps the most widely-used method for
estimating out-of-sample prediction error and comparison of statistical
models. By fitting the model on the training data set and then evaluating it on
the hold-out testing set, the over-optimism of using data twice is
avoided. Furthermore, attempts have been made to use cross-validated objective
functions for statistical inference [@craven1978smoothing;@seeger2008cross],
thus integrating out-of-sample prediction error estimation and model selection
into one step.

In this chapter, I will discuss several challenges I encounter in using
cross-validation predictive accuracy in evaluating and selecting among
multilevel models, specifically in binary classification models. The first
challenge is the lack of clear protocol for the cross-validation procedure: to
truly test the model, the holdout set cannot be a simple random sample of the
data but instead needs to have some multilevel structure itself, so that entire
groups as well as individual observations are held out.  Hierarchical
cross-validation can be performed in the context of particular applications
[@PriceGelmanNero1996] but it is not clear how best to subsample structured data
for cross-validation in a general way.  The second challenge is that, in
multilevel models, the observed loss function for data-level cross-validation
can be so close to flat that the cross-validation estimates of prediction errors
under candidate models can be swamped by random fluctuations.

I focus on the second of these concerns, demonstrating the limitations of
prediction error in the context of a set of multilevel models fit to a large
cross-tabulated national survey.  An innovative aspect of our analysis is that
we evaluate separately on 71 different survey responses, taking each in turn as
the outcome in a comparison of regression models.  This allows us to construct a
relatively large corpus of data out of a single survey.

This chapter is a joint work with Andrew Gelman, and mostly based on
[@wang2014difficulty].

# Multilevel Models and Survey Research #

There are two types of survey researchers, as identified by the classic book
"Survey Erros and Survey Costs" [@groves2004survey], the _describers_, who "use
surveys to descxribe characteristics of a fixed population", and the _modelers_,
who "seek to identify causes of phenomena constatly occuring in a society". The
latter group developed models to genertae less biased estimates, as a result of
using more data and handling more inherent structure within the data. Multilevel
models, an example of the _modeler_ approach, are effective in survey research,
as partial pooling can yield accurate state-level estimates from national polls
[@gelman2007data]. Multilevel models have been successfully applied both to
representative and nonrepresentative surveys to obtain accurate small-area
estimation and prediction [@FayHerriot:1979;@lax_2009;@ghitza_2013;@wang_2014],
and the practical application of such methods is currently being actively
discussed in social science research [@buttice_2013;@lax_2013]. In the present
paper, we conduct model selection procedures based on $k$-fold cross-validation
and find that under this framework, the improvement of multilevel models over
classical models is surprisingly small when measured on the scale of prediction
error. Furthermore, we demonstrate that this lack of notable improvement is
related to the sample size and data structure by repeating the analysis on
simulated data sets that vary in terms of these two factors.

Our results illustrate that under multilevel structure, it could be tricky to
use cross-validation in model selection, as the size of the data and how balanced
the structure is heavily affect the relative performance of the models.

In the next section, I will present a fully Bayesian model comparison framework,
a preparation for the real data analysis.

# Model Assessment and Selection via Cross-Validation #

## Predictive Loss ##

I start with a loss function $l(\tilde y, a)$ corresponding to the inferential
action $a_M$ based on a model $M$, in face of future observations $\tilde
y$. The available data, typically consisting of predictors $x$ and outcomes $y$,
are labeled as $D$. The corresponding predictive loss is then,

\begin{equation}
\label{eq:explossgeneral}
PL(p^t, M, D)=E_{p^t}l(\tilde y,
a_M)=\int l(\tilde y, a_M) p^t(\tilde y)d\tilde y
\end{equation}

\noindent where $p^t(\cdot)$ is the true distribution from which the future observations
$\tilde y$ are generated.

The predictive loss is affected by the form of the action $a_M$, the loss
function $l$, and the data $D$. For example, $a_M$ could be the mean of the
posterior predictive distribution and $l$ the mean square error loss. However,
it is often convenient and theoretically desirable to use the whole posterior
predictive distribution as the inferential action and a logarithmic loss
function.  In addition, using the whole posterior predictive distribution has a
Bayesian justification, as it reflects the full inferential uncertainty
conditional on the model [@Vehtari2012a]. Substituting the choice of $a_M$
and $l$ into (\ref{eq:explossgeneral}) yields,

\begin{align}
  \begin{split}
  \label{eq:logloss}
  PL(p^t, M, D)&=E_{p^t}[-\log p(\tilde y|D, M)]\\ 
  &=-\int p^t(\tilde y) \log p(\tilde y|D, M) d\tilde y
  \end{split}
  \end{align}
  
\noindent This quantity is central to predictive model selection. The
fundamental difficulty in estimating it is that the true distribution
$p^t(\cdot)$ is unknown.

Another important quantity arises when we approximate the true distribution with
the empirical distribution, which gives the training loss,

\begin{align}
  \begin{split}
  \label{eq:trloss}
  TL(M, D)&=-\int \log p(y|D, M) d\hat{F}(y)\\
  &=-\,\frac{1}{N}\sum_{y\in D}\log p(y | D, M).
  \end{split}
\end{align}
  
The training loss uses the same data for both estimation and evaluation and so
in general underestimates prediction error.

## Prediction Error ##

With (\ref{eq:logloss}), the model selection task is straightforward. Among the
candidate models, the best model under this framework is the one that minimizes
the predictive loss:

\begin{equation}
  \label{eq:minimizer}
  - \min_{M} \int \!p^t(\tilde y) \log p(\tilde y|D, M) d\tilde y,
  \end{equation}
  
which has a lower bound, $-\!\int\! p^t(\tilde y) \log p^t(\tilde y) d\tilde y$,
which is the entropy of the true distribution. It is often more informative to
look at the excess of the predictive loss over this lower bound, as shown in
(\ref{eq:preerror}). I label this quantity as the prediction
error. Conceptually, the prediction error indicates how far the posterior
predictive distribution is from the oracle, and it is the Kullback-Leibler
divergence between the posterior predictive distribution of the candidate model
and the true generative model. As its form suggests, the prediction error is the
difference between log posterior predictive density and log true predictive
density, averaged over the true predictive distribution,

\begin{align}
\begin{split}
  \label{eq:preerror}
    &PE(p^t, M, D)= PL(p^t, M, D) - LB(p^t) \\  
               &=-\int p^t(\tilde y) \log p(\tilde y|D, M) d\tilde y+\int
               p^t(\tilde y) \log p^t(\tilde y) d\tilde y.
               \end{split}
\end{align}
               
So to estimate the prediction error, we need to estimate the two terms in
(\ref{eq:preerror}).

## $k$-fold Cross-Validation for Estimating Predictive Loss ##

In the predictive framework, the central obstacle of estimating the predictive
loss (\ref{eq:logloss}) is that the future observations are not available. One
thread of research attempts to estimate and correct the bias introduced by
reusing the sample and thus gives rise to various information criteria, whose
validity hinges on a number of assumptions and simplifications. Another thread
of research is to use hold-out data for testing, thus making training and
testing data independent. This leads to a variety of resampling procedures,
including leave-one-out cross-validation, $k$-fold cross-validation, Monte Carlo
cross-validation, and bootstrapping. In practice, $k$-fold cross-validation is
popular due to its computational convenience and stability
[@kale2011cross]. Formally, the $k$-fold cross-validation of the predictive loss
is given by

\begin{align}
\begin{split}
  \label{eq:xvalesti}
  \widehat{PL}^{\text{CV}}(M, D) &=-\,\frac{1}{N}\sum_{k=1}^K\sum_{i\in
    \text{test}_k}\log p(y_i|D^k, M)\\
  &=-\,\frac{1}{N}\sum_{i=1}^N\log
  p(y_i|D^{(\backslash i)}, M),
\end{split}
\end{align}

\noindent where $D^k$ represents the $k$\textsuperscript{th} training set,
$\text{test}_k$ represents the $k$\textsuperscript{th} testing set under the
random partition and $D^{(\backslash i)}$ denotes the training set that excludes
the $i$\textsuperscript{th} observation. Because $k$-fold cross-validation does
not use all the data, the prediction error estimates are biased, but in the
cases where there are relatively few predictors, this bias is small
[@burman1989comparative].

The practical impediment of using cross-validation is the computational burden:
with $k$-fold cross-validation, we need to fit the model $k$ times.  However, in
many cases it is possible to perform the $k$ steps in parallel.

The problem remains of estimating the second term in (\ref{eq:preerror}), namely
the lower bound of predictive loss. In this paper, we use the in-sample training
loss $TL(M_{\text{s}}, D)$ of the saturated model $M_s$ as the surrogate for the
lower bound. So the estimated prediction error is

\begin{align}
\begin{split}
  \label{eq:esti_preerror}
  &\widehat{PE}(M, D)=\widehat{PL}^{\text{CV}}(M,D)-TL(M_{\text{s}},D)\\
  &= -\,\frac{1}{N}\sum_{i=1}^N\log p(y_i|D^{(\backslash i)},
  M)+\frac{1}{N}\sum_{y\in D}\log p(y | D, M_{\text{s}}).
\end{split}
\end{align}

# Cross-Validation of Structured Data  #

Standard cross-validation assumes that data are independent and with no
distributional differences between the training and testing sets. For structured
data, it is not always clear how best to perform this
partition. [@burman1994cross] discusses a modification of ordinary
cross-validation procedure for stationary time series. In this paper, we focus
on the cross-tabulated structure, which is the characteristic of survey data
with discrete responses. In an unbalanced cross-tabulated data set, simple
random sampling might result in undersampling of small cells. Thus, we adopt a
stratified sampling approach to guarantee that each cell is partitioned into a
training part and a testing part. Another possibility is to perform a cluster
sampling and train the model on some cells and test the fitted model on
others. This approach is related to transfer learning [@pan2010survey]. In the
analysis of survey data, the focus is mostly on the existing cells rather than
on hypothetical new cells, and so we only discuss cross-validation using
stratified sampling on structured data.


# Comparing Multilevel Models for Binary Survey Outcomes #

The 2006 Cooperative Congressional Election Survey, the example data set in this
paper, is a national stratified sample of size 30,000 that includes a wide
variety of response outcomes, thus providing an ideal setting to evaluate
cross-validation. Although various demographic predictors are available in this
data set, we keep our model simple by using only two predictors, state and
income. Under this setting, the multilevel model is the preferred model over no
pooling (saturated model) or complete pooling (additive model). On one hand, the
saturated model will trigger overfitting. On the other hand, income and state
are known to have strong interactions when predicting electoral choice
[@redstate], so the additive model must be substantively inadequate.

## Complete Pooling, No Pooling, and Partial Pooling Models ##

Bayesian multilevel modeling is a natural choice for analyzing cross-tabulated
data. When the data provide many explanatory variables, and thus a potentially
complex cross-tabulated structure, it is difficult to model the interactions
among explanatory variables in classical models, since each single cell is
getting sparser and the estimates become unstable. By borrowing strength across
cells, a multilevel model (or, alternatively, some other structured model such as
a Gaussian process) can produce stable estimates even for cells that have few
observations and thus can be viewed as a multivariate regression or interpolation
procedure..

We develop our model on a simple two-way cross-tabulation of survey data, with
state and income as the two explanatory variables, having $J_1$ and $J_2$ levels
respectively.\footnote{For the 2006 Cooperative Congressional Election Survey
  data set, there are 50 states ($J_1=50$), and 5 income levels ($J_2=5$),
  including less than \$20,000, \$20,000-\$40,000, \$40,000-\$75,000,
  \$75,000-\$150,000, and \$150,000+.} We assume no continuous predictors in our
model. Let $N$ be the total sample size of the survey, then the array of cell
counts follows a multinomial distribution,
\begin{equation*}\bm{N}\sim \text{Multinomial}(N, \bm{p})\end{equation*},
where

\begin{align*}
  \bm{N}&=(N_{j_1j_2})_{J_1\times J_2},\\
  \bm{p}&=(p_{j_1j_2})_{J_1\times J_2}.
\end{align*}
  
\noindent The population is thus divided into $J_1\times J_2$ cells. We
constrain our discussion to binary outcomes. Then for a respondent in cell
$(j_1, j_2)$, the probability that he or she gives a positive response is
$\pi_{j_1j_2}$, which is modeled using logistic regression:

\begin{equation*}
\text{logit}(\pi_{j_1j_2})=\bm Z\bm\beta,
\end{equation*}

\noindent in which $\bm Z$ is the covariate vector and $\bm\beta$ includes the
main and interaction effects. Since our goal of inference is on cell proportions
$\pi_{j_1j_2}$ rather than cell assignment probabilities $p_{j_1j_2}$, we treat
$p_{j_1j_2}$ as fixed throughout.

Under this setup, we consider three models:

\begin{itemize}
\item Complete pooling of interactions:
\begin{equation*}    \pi_{j_1j_2}=\text{logit}^{-1}\left(\beta^{\text{state}}_{j_1}+\beta^{\text{inc}}_{j_2}\right)\end{equation*}
\item
  No pooling:
\begin{equation*}    \pi_{j_1j_2}=\text{logit}^{-1}\left(\beta^{\text{state}}_{j_1}+\beta^{\text{inc}}_{j_2}+\beta^{\text{state*inc}}_{j_1j_2}\right)\end{equation*}

\item
  Partial pooling:
  \begin{equation*}  \pi_{j_1j_2}=\text{logit}^{-1}\left(\beta^{\text{state}}_{j_1}+\beta^{\text{inc}}_{j_2}+\beta^{\text{state*inc}}_{j_1j_2}\right) \end{equation*} with
    $\beta^{\text{state inc}}_{j_1j_2}\stackrel{i.i.d.}{\sim} \mbox{N}(0,\sigma^2)$
   \noindent where the scale parameter $\sigma$ is estimated from the data (with a separate value for each survey outcome).
\end{itemize}
    
Although nonparametric multilevel modeling, both in the Bayesian
[@hjort2010bayesian] and the frequentist [@ruppert2003semiparametric]
perspectives, have been under rapid development, we adopt a linear parametric
specification for the multilevel model, because linear parametric models are
still the standard specification, and software that fit the routine linear
parametric models are widely available and easily accessible to practitioners.
In the remaining sections of this paper, we compare the prediction error of
these three models under various real data and simulation settings.

We recognize that multilevel models in big-data applications can be much more
complicated [@ghitza_2013]; we use a relatively simple example here to explore
the basic ideas.

## Computation ##

Ideally we want to do full Bayesian inference on our model, but for computational
reasons we are currently using an approximate marginal posterior mode estimate
provided by blme [@blme] in R, which is an extension of the widely-used lme4
[@lme4] package. The lme4 package approximately integrates out the random
effects to obtain an approximate marginal MLE of the scale parameter and the
fixed effects. However, modal estimates can end up on the boundary due to
sampling variability [@chung2013nondegenerate], which in our case makes the
partial pooling model reduce to complete pooling. In blme, the scale parameter
$\sigma$ is also given a gamma prior with shape parameter 2.5 and rate parameter
0. The gamma prior is used to regularize the prior of the scale and pull the
estimates of the interactions away from zero, a situation that often happens in
modal estimation. We have developed an R package, mrp [@mrp], to streamline
the multilevel model fitting and cross-validation procedure.


## Estimation Procedure ##
For each outcome, we fit a multilevel logistic regression model, with additive,
fully-interacted, and multilevel models. We use $5$-fold cross-validation to
estimate predictive loss (using more folds gives essentially identical results).
We estimate the lower bound using the training loss of the saturated model.

Under the aforementioned setting, the cross-validation loss estimate is,

\begin{align*}
  \label{eq:CV4MultiWaySurvey}
  &\widehat{PL}^{\text{CV}}(M, D) =-\,\frac{1}{N}\sum_{k=1}^K\sum_{j\in \text{test}_k}\log p(y_j|D^k, M)\\
  & = -\,\frac{1}{N}\sum_{k=1}^K\sum_{i,j} [y^{\text{test}_k}_{ij}\log\hat\pi_{ij}^{D^k}+(n^{\text{test}_k}_{ij}- y^{\text{test}_k}_{ij})\log(1-\hat\pi_{ij}^{D^k})]\\
  & = -\,\frac{1}{N}\sum_{i,j}\sum_{k=1}^K[ y^{\text{test}_k}_{ij}\log\hat\pi_{ij}^{D^k}+(n^{\text{test}_k}_{ij}- y^{\text{test}_k}_{ij})\log(1-\hat\pi_{ij}^{D^k})]\\
  & = -\,\frac{1}{N}\sum_{i,j}\big[y_{ij}\overline{\log\hat{\pi}_{ij}} +(n_{ij}- y_{ij})\overline{\log(1-\hat\pi_{ij})}\big]\\
  & = -\,\sum_{i,j}\frac{n_{ij}}{N}\big[\pi_{ij}\overline{\log\hat\pi_{ij}} +(1- \pi_{ij})\overline{\log(1-\hat\pi_{ij})}\big],
\end{align*}
  
\noindent in which $n_{ij}^{\text{test}_k}$ is the number of respondents in cell $(i,j)$
of the $k$-th testing set, $y_{ij}^{\text{test}_k}$ is the number of respondents
who answered yes in cell $(i,j)$ of the $k$-th testing set, correspondingly,
$n_{ij}$ and $y_{ij}$ are the numbers of total respondents and respondents who
answered yes in cell $(i,j)$, $\hat\pi_{ij}^{D^k}$ is the estimated $\pi_{ij}$
using the $k$-th training data set, and $\overline{\log\hat\pi_{ij}}$ is the
weighted average log posterior proportion from each fold,
$\left(\sum_{k=1}^Ky^{\text{test}_k}_{ij}\log\hat\pi_{ij}^{D^k}
\right)\big/y_{ij}$, and $\overline{\log(1-\hat\pi_{ij})}$ has the similar
form. The cross-validation loss estimate is approximately a measure of loss
under cell proportion distribution $(\exp(\overline{\log\hat\pi_{ij}}),
\exp(\overline{\log(1-\hat\pi_{ij})}))$ (here we say ``approximately'' because
these two probabilities do not in general add up to 1). The quick calculation in
section 1.2 suggests that we should expect to see only small improvements in
cross-validation loss even from substantively important model improvements.

# Results #

## Prediction Errors for a Corpus of Outcomes ##

\begin{figure*}
  \centering
  \includegraphics[width=.85\textwidth]{alloutcomesx1.pdf}
  \caption{\em Measure of fit (estimated prediction error) for all response outcomes
    in the 2006 Cooperative Congressional Election Survey. Outcomes are ordered by the lower bound
    (in-sample loss of the saturated model). The no pooling model
    gives a bad fit.  Partial pooling does best but in most cases is almost indistinguishable from complete pooling under the cross-validation criterion.}
  \label{fig:figx1}
\end{figure*}

We begin by estimating the prediction errors of all outcomes in the survey. The
results are shown in Figure~\ref{fig:figx1}. The $x$-axis is ordered by the
in-sample training loss of the saturated model $TL(M_{\text{s}},D)$, which we
use as a surrogate for a lower bound of predictive loss. For complete pooling
and partial pooling, the prediction error stays stable across different
outcomes, while the no pooling model has huge prediction error for outcomes with
small lower bounds. This finding makes sense since these are the settings where
overfitting is most severe (saturated models achieve the lowest in-sample
training error). However, the difference in prediction error between complete
pooling and partial pooling seems negligible. Partial pooling is giving
essentially the same result as complete pooling, at least according to
cross-validation on individual survey responses.

\begin{figure*}
  \centering
  \includegraphics[width=.4\textwidth]{esti_all_bglmer}
  \includegraphics[width=.4\textwidth]{esti_big_bglmer}
  \caption{\em Left panel: Cell proportion estimates for three models of vote
    intention. Each line is a state. The
    partial pooling model pools so much that it is indistinguishable from
    complete pooling. Right panel: The same estimates for the 10 most populous
    states. Still, partial pooling estimates are similar to complete pooling
    estimates.}
  \label{fig:234}
\end{figure*}

This seems to suggest that partial pooling does not have enough information to
estimate cell-to-cell variation, thus giving an overly conservative
estimate. Indeed, when we plot the estimates of $\pi_{j_1j_2}$ for one particular
outcome, vote preference for in the congressional election (see the left panel of
Figure~\ref{fig:234}), the estimates from partial pooling are almost identical to
those from complete pooling. Even for populous states where, because of their
large sample size, the amount of partial pooling should be small, there are no
major differences between estimates from partial pooling model and estimates from
complete pooling model (see the right panel of Figure~\ref{fig:234}). This
pattern is consistent across different outcomes.

Although we believe partial pooling is intrinsically better than complete
pooling, it seems that the given data are not sufficient for the partial pooling
model to pick up the interaction and unpool the estimates appropriately. It is a
result of the particular characteristics of this data set?  There are three
factors determining the structure of the data that might affect the extent of
pooling of the model. First is the sample size. If we increase the sample size to
a sufficiently large level, the partial pooling model will be able to partially
pool the estimates to an appropriate amount. As sample size grows, the no pooling
model will eventually have the same performance as partial pooling, and it might
be interesting to see at what point the saturated model becomes acceptable. The
second factor affecting the relative performance of the different models is the
size of the interactions that are being estimated, and the third factor is the
level of imbalance in the hierarchical structure. Survey data classified by
demographic and geographic predictors are typically highly unbalanced due to the
long tails of sizes typical in taxonomic structures [@Mandelbrot:1955]. For
example, the 2006 CCES includes 3,637 respondents from California but only 131
from Arkansas. This unbalanced structure will affect the amount of pooling
performed by a multilevel model.

In the following subsections, we conduct simulations that vary sample size and
the structure of the cells to investigate how these factors affect the relative
performance of the three models as captured by cross-validation.

## How Sample Size Changes the Dynamics ##

\begin{figure}[p]
  \centering
  \includegraphics[width=.45\textwidth]{outcome234.pdf}
  \caption{\em Estimated prediction error of all response outcomes for augmented
    data sets. From top to bottom, the data sets have 2, 3, and 4 times as many
    data points as the original data set. The outcomes are ordered by the
    in-sample predictive loss. As sample size grows, complete pooling
    gradually gets worse and no pooling gets better.}
  \label{fig:figx234}
\end{figure} 
We artificially augment the data set by combining the data set with itself. New
data sets with sample size that are 2, 3 and 4 times as large are generated.
This augmentation still maintains the same level of interactions and cell
structure as those of the original data. Then we estimate the prediction errors
for all outcomes for the three models. Results are plotted in
Figure~\ref{fig:figx234}. As we expected, as sample size grows, the prediction
error of complete pooling model, which is essentially a wrong model, dominates
the other two; while the prediction error of no pooling model keeps
decreasing. When the sample size is 4 times as large as the original data set, no
pooling model has almost the same prediction error as partial pooling model. This
makes sense, since the problem of overfitting eventual goes away if we have
sufficiently large sample size and fixed model structure.
\begin{figure*}
  \centering
  \includegraphics[width=.85\textwidth]{hourvote.pdf}
  \caption{\em Prediction error of the three models as sample size grows. The
    outcome under consideration is partisan vote preference in the upcoming
    congressional election. By this criterion, partial pooling and complete
    pooling perform similarly until sample size exceeds 50,000.}
  \label{fig:hourvote}
\end{figure*}

These results suggest that for a fixed data structure, partial pooling decisively
outperforms no pooling and complete pooling only for a certain window of sample
sizes. To have a closer look at the range of the window, we look at one
particular outcome, the vote preference in the upcoming election for the
U.S. House of Representatives. We augment the sample size and plot the relative
performance of the three models in Figure~\ref{fig:hourvote}. Partial pooling
model is noticeably better than complete pooling in this setup when the total
sample size exceeds larger than 50,000. Other outcomes have similar patterns.

## Balancedness of the Hierarchical Structure ##


One possible explanation for the steep learning curve of the partial pooling
model is the highly unbalanced structure of the data. Although we have 50 states,
the estimate of the covariance of the state random effects might not be reliable
since some of the states have small sample sizes. To see how the balancedness of
the structure affects the model, we simulate a data set based on partial pooling
estimates from the original data set, but make each demographic-geographic cells
of roughly the same size. The overall sample size is the same as that of the real
data. Relative performance of the three models for all outcomes is plotted in
Figure~\ref{fig:figbal}. The graph shows that with balanced hierarchical
structure, at the same sample size and amount of interaction, partial pooling
kicks in much more quickly. Thus partial pooling is consistently better than
complete pooling in this scenario. As in the previous analysis, we also look at
the relative performance of the three models as sample size grows. The results
are plotted in Figure~\ref{fig:hourvote_bal}.

\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{alloutcomesxbal.pdf}
  \caption{\em Measure of fit (prediction error) for all outcomes, ordered by
    in-sample training loss. The data set is simulated from real data set, and
    has the same sample size in total as the real data set, but keeping all
    demographic-geographic cells balanced. In this case, complete pooling model
    has much higher prediction errors than no pooling and partial
    pooling. Partial pooling is slightly but consistently better than no
    pooling. In particular, no pooling model has huge prediction error for
    outcomes that have smaller in-sample training loss.}
  \label{fig:figbal}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{hourvote_bal.pdf}
  \caption{\em Prediction error of the three models as sample size grows under the
    simulated balanced data set. The outcome under consideration is the vote for
    the Republican candidate in the U.S House of Representatives. Partial
    pooling has the lowest prediction error when sample size is under 70,000.}
  \label{fig:hourvote_bal}
\end{figure} 

# Discussion #

Cross-validation is an important tool used to evaluate a wide variety of
statistical methods and has been widely used in model comparison when predictive
power is of concern. Some theoretical treatments have pointed out situations
where cross-validation might have problems. For example, [@shao1993linear] shows
that, under the frequentist setting, using leave-one-out cross-validation for
linear model variable selection is not consistent.  However, the simplicity and
transparency of cross-validation gives it a near-universal appeal. In this
paper, we investigate the sensitivity of cross-validation as a model comparison
instrument in a cross-tabulated multilevel survey data set.

We set up the model selection problem, considering three models for these structured data:  the classical models of complete pooling and no pooling,
and a Bayesian multilevel model.  The multilevel
model captures important interactions that are not included in the complete
pooling model, while at the same time avoiding the inevitable overfitting from
the no pooling model. However, the improvement of the multilevel model as given
by cross-validation is surprisingly tiny, almost negligible to unsuspecting eyes.
The problem is that improved fits with binary data yield minuscule improvements
in log loss, in moderate sample sizes nearly indistinguishable from noise even if
the improved estimates are substantively important when aggregated (for example,
state-level public opinion).  Simulations based on real data show that
sample size and structure of the cross-tabulated cells play important roles in
the relative margins of different models in cross-validation based model
selection. Caution should be exercised in applying prediction error for model
selection with structured data.



# Bibliography #


